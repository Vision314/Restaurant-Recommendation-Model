{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Data Analysis Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/xpbsmtsx0xzbs7_6z34b_5yr0000gn/T/ipykernel_18841/1002596979.py:5: DtypeWarning: Columns (15,16,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  orders = pd.read_csv('../data/orders.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "train_full\n",
      "(5802400, 73)\n",
      "train_customers\n",
      "(34674, 8)\n",
      "train_locations\n",
      "(59503, 5)\n",
      "orders\n",
      "(135303, 26)\n",
      "vendors\n",
      "(100, 59)\n",
      "sample_submission\n",
      "(1672000, 2)\n",
      "Data columns:\n",
      "train_full\n",
      "Index(['customer_id', 'gender', 'status_x', 'verified_x', 'created_at_x',\n",
      "       'updated_at_x', 'location_number', 'location_type', 'latitude_x',\n",
      "       'longitude_x', 'id', 'authentication_id', 'latitude_y', 'longitude_y',\n",
      "       'vendor_category_en', 'vendor_category_id', 'delivery_charge',\n",
      "       'serving_distance', 'is_open', 'OpeningTime', 'OpeningTime2',\n",
      "       'prepration_time', 'commission', 'is_akeed_delivering',\n",
      "       'discount_percentage', 'status_y', 'verified_y', 'rank', 'language',\n",
      "       'vendor_rating', 'sunday_from_time1', 'sunday_to_time1',\n",
      "       'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
      "       'monday_to_time1', 'monday_from_time2', 'monday_to_time2',\n",
      "       'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
      "       'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1',\n",
      "       'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1',\n",
      "       'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2',\n",
      "       'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
      "       'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1',\n",
      "       'saturday_from_time2', 'saturday_to_time2', 'primary_tags',\n",
      "       'open_close_flags', 'vendor_tag', 'vendor_tag_name', 'one_click_vendor',\n",
      "       'country_id', 'city_id', 'created_at_y', 'updated_at_y', 'device_type',\n",
      "       'display_orders', 'location_number_obj', 'id_obj',\n",
      "       'CID X LOC_NUM X VENDOR', 'target'],\n",
      "      dtype='object')\n",
      "train_customers\n",
      "Index(['akeed_customer_id', 'gender', 'dob', 'status', 'verified', 'language',\n",
      "       'created_at', 'updated_at'],\n",
      "      dtype='object')\n",
      "train_locations\n",
      "Index(['customer_id', 'location_number', 'location_type', 'latitude',\n",
      "       'longitude'],\n",
      "      dtype='object')\n",
      "orders\n",
      "Index(['akeed_order_id', 'customer_id', 'item_count', 'grand_total',\n",
      "       'payment_mode', 'promo_code', 'vendor_discount_amount',\n",
      "       'promo_code_discount_percentage', 'is_favorite', 'is_rated',\n",
      "       'vendor_rating', 'driver_rating', 'deliverydistance', 'preparationtime',\n",
      "       'delivery_time', 'order_accepted_time', 'driver_accepted_time',\n",
      "       'ready_for_pickup_time', 'picked_up_time', 'delivered_time',\n",
      "       'delivery_date', 'vendor_id', 'created_at', 'LOCATION_NUMBER',\n",
      "       'LOCATION_TYPE', 'CID X LOC_NUM X VENDOR'],\n",
      "      dtype='object')\n",
      "vendors\n",
      "Index(['id', 'authentication_id', 'latitude', 'longitude',\n",
      "       'vendor_category_en', 'vendor_category_id', 'delivery_charge',\n",
      "       'serving_distance', 'is_open', 'OpeningTime', 'OpeningTime2',\n",
      "       'prepration_time', 'commission', 'is_akeed_delivering',\n",
      "       'discount_percentage', 'status', 'verified', 'rank', 'language',\n",
      "       'vendor_rating', 'sunday_from_time1', 'sunday_to_time1',\n",
      "       'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
      "       'monday_to_time1', 'monday_from_time2', 'monday_to_time2',\n",
      "       'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
      "       'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1',\n",
      "       'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1',\n",
      "       'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2',\n",
      "       'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
      "       'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1',\n",
      "       'saturday_from_time2', 'saturday_to_time2', 'primary_tags',\n",
      "       'open_close_flags', 'vendor_tag', 'vendor_tag_name', 'one_click_vendor',\n",
      "       'country_id', 'city_id', 'created_at', 'updated_at', 'device_type',\n",
      "       'display_orders'],\n",
      "      dtype='object')\n",
      "sample_submission\n",
      "Index(['CID X LOC_NUM X VENDOR', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import all training data and vendor, order, and customer data\n",
    "train_full = pd.read_csv('../data/train_full.csv', low_memory=False)\n",
    "train_customers = pd.read_csv('../data/train_customers.csv')\n",
    "train_locations = pd.read_csv('../data/train_locations.csv')\n",
    "orders = pd.read_csv('../data/orders.csv')\n",
    "vendors = pd.read_csv('../data/vendors.csv')\n",
    "sample_submission = pd.read_csv('../data/SampleSubmission.csv')\n",
    "\n",
    "# List of all datasets and names\n",
    "data_collection = [train_full, train_customers, train_locations, orders, vendors, sample_submission]\n",
    "data_names = ['train_full', 'train_customers', 'train_locations', 'orders', 'vendors', 'sample_submission']\n",
    "\n",
    "# Display shape of each dataset\n",
    "print(\"Data shapes:\")\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(name)\n",
    "    print(data.shape)\n",
    "    \n",
    "# Display column names for each data set\n",
    "print(\"Data columns:\")\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(name)\n",
    "    print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- train_full is a combination of SampleSubmissions, vendors, and orders with the ultimate result being 1. In our scenario where we want to recommend a restaurant to a person based off previous order history, we may be able to ignore many of the features presented. However, the one unique column is target. By assumption, it would seem that we need to classify yes/no (1/0) based on the rest of the information given here. \n",
    "- train_customers is mostly useless. The majority of the information is either missing or very similar. There could be some analysis given gender (22k) or dob (3k).\n",
    "- train_locations can potentially be useful. It lists the number of locations each customer has, although the majority only has one location and they are unlabeled so we would need to make assumptions.\n",
    "- orders will defitinely be helpful since this is the basis of our project.\n",
    "- vendors will also be helpful in understanding the type of restaurant and being able to filter out recommendations based on location and time of day.\n",
    "- After further analysis, it is clear that sample_submission is the given file containing the customer, order location (home, work, other), and a vendor. Given this file we need to decide if the customer would order here or not. (1/0)\n",
    "- This brings up the question if we would be then only focusing on if someone would order from a vendor, if we are trying to recommend a number of vendors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train_full\n",
      "commission\n",
      "is_akeed_delivering\n",
      "language\n",
      "open_close_flags\n",
      "one_click_vendor\n",
      "country_id\n",
      "city_id\n",
      "display_orders\n",
      "Dataset: train_customers\n",
      "language\n",
      "Dataset: train_locations\n",
      "Dataset: orders\n",
      "Dataset: vendors\n",
      "commission\n",
      "is_akeed_delivering\n",
      "language\n",
      "open_close_flags\n",
      "one_click_vendor\n",
      "country_id\n",
      "city_id\n",
      "display_orders\n",
      "Dataset: sample_submission\n",
      "target\n",
      "Dataset: train_full\n",
      "is_open\n",
      "status_y\n",
      "verified_y\n",
      "device_type\n",
      "Dataset: train_customers\n",
      "Dataset: train_locations\n",
      "Dataset: orders\n",
      "Dataset: vendors\n",
      "is_open\n",
      "status\n",
      "verified\n",
      "device_type\n",
      "Dataset: sample_submission\n"
     ]
    }
   ],
   "source": [
    "# For each dataset, we check if the number of unique values is one for each column. If so, we print the column name and add to list.\n",
    "\n",
    "drop_columns = []  \n",
    "\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(f'Dataset: {name}')\n",
    "    for column in data.columns:\n",
    "        # Check if the column only has one unique value\n",
    "        if data[column].nunique() == 1:\n",
    "            drop_columns.append(column)\n",
    "            print(column)\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "            \n",
    "                \n",
    "# Now we check if we drop all missing values, if the number of unique values is one for each column. If so, we print the column name.\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(f'Dataset: {name}')\n",
    "    temp_data = data.dropna()\n",
    "    for column in temp_data.columns:\n",
    "        # Check if the column only has one unique value\n",
    "        if temp_data[column].nunique() == 1:\n",
    "            drop_columns.append(column)\n",
    "            print(column)\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full datasets: \n",
    "train_full: \n",
    "- commission\n",
    "- is_akeed_delivering\n",
    "- language\n",
    "- open_close_flags\n",
    "- one_click_vendor\n",
    "- country_id\n",
    "- city_id\n",
    "- display_orders\n",
    "train_customers:\n",
    "- language\n",
    "vendors: \n",
    "- commission\n",
    "- is_akeed_delivering\n",
    "- language\n",
    "- open_close_flags\n",
    "- one_click_vendor\n",
    "- country_id\n",
    "- city_id\n",
    "- display_orders\n",
    "\n",
    "Dropped missing values (unique from previous)\n",
    "train_full:\n",
    "- is_open\n",
    "- status_y\n",
    "- verified_y\n",
    "- one_click_vendor\n",
    "- device_type\n",
    "Dataset: vendors\n",
    "- is_open\n",
    "- status\n",
    "- verified\n",
    "- one_click_vendor\n",
    "- device_type \n",
    "\n",
    "Notes:\n",
    "- There are a total of 8 columns that only contain one unique entry between train_full and vendors. Train_customers contains one of these as well. \n",
    "- When dropping missing entries, we have another 5 columns between train_full and vendors with only one unique entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_akeed_delivering', 'display_orders', 'verified_y', 'device_type', 'status_y', 'verified', 'status', 'target', 'city_id', 'one_click_vendor', 'is_open', 'country_id', 'language', 'commission', 'open_close_flags']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Analysis of the columns that have only one unique value\n",
    "drop_columns = list(set(drop_columns))\n",
    "print(drop_columns)\n",
    "print(len(drop_columns))\n",
    "\n",
    "# To drop the columns, go above and uncomment the following line:\n",
    "# data.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data shapes:\n",
      "(78254, 61)\n",
      "    customer_id gender  status_x  verified_x         created_at_x  \\\n",
      "56      TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "227     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "362     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "370     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "404     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "\n",
      "            updated_at_x  location_number location_type  latitude_x  \\\n",
      "56   2018-02-07 19:16:23                0          Work    -96.4400   \n",
      "227  2018-02-07 19:16:23                2           NaN     -0.1287   \n",
      "362  2018-02-09 12:04:41                0          Home     -0.1755   \n",
      "370  2018-02-09 12:04:41                0          Home     -0.1755   \n",
      "404  2018-02-09 12:04:41                1          Home      0.1912   \n",
      "\n",
      "     longitude_x  ...  saturday_to_time2             primary_tags  \\\n",
      "56        -67.20  ...           23:45:00                      NaN   \n",
      "227       -78.56  ...                NaN   {\"primary_tags\":\"462\"}   \n",
      "362       -78.56  ...                NaN   {\"primary_tags\":\"180\"}   \n",
      "370       -78.56  ...                NaN  {\"primary_tags\":\"1088\"}   \n",
      "404       -78.60  ...           23:59:00    {\"primary_tags\":\"32\"}   \n",
      "\n",
      "               vendor_tag                                    vendor_tag_name  \\\n",
      "56   1,5,8,57,30,27,24,16  American,Burgers,Desserts,Donuts,Fries,Pasta,S...   \n",
      "227             2,8,91,10               Arabic,Desserts,Free Delivery,Indian   \n",
      "362                 41,57                                       Cakes,Donuts   \n",
      "370        60,38,61,16,36  Coffee,Fresh Juices,Hot Chocolate,Sandwiches,S...   \n",
      "404                     5                                            Burgers   \n",
      "\n",
      "            created_at_y         updated_at_y  location_number_obj  id_obj  \\\n",
      "56   2019-04-30 16:15:30  2020-04-07 18:45:33                    0     237   \n",
      "227  2018-10-24 18:11:35  2020-04-07 23:51:01                    2     113   \n",
      "362  2019-05-31 14:33:35  2020-04-06 19:57:22                    0     274   \n",
      "370  2019-07-02 14:50:31  2020-03-31 13:23:57                    0     303   \n",
      "404  2018-05-17 22:12:38  2020-04-05 15:57:41                    1      28   \n",
      "\n",
      "    CID X LOC_NUM X VENDOR target  \n",
      "56       TCHWPBT X 0 X 237      1  \n",
      "227      TCHWPBT X 2 X 113      1  \n",
      "362      ZGFSYCZ X 0 X 274      1  \n",
      "370      ZGFSYCZ X 0 X 303      1  \n",
      "404       ZGFSYCZ X 1 X 28      1  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtering to further understand dataset\n",
    "filtered_full = train_full[train_full['target'] == 1]\n",
    "\n",
    "print(\"Filtered data shapes:\")\n",
    "print(filtered_full.shape)\n",
    "print(filtered_full.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
