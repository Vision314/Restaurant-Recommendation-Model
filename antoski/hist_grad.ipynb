{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement histogram gradient boosting classifier tree. \n",
    "\n",
    "Parameters below: \n",
    "loss='log_loss', *, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, class_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to consider the cusomter and vendor IDs because they are categorical data. HistGradBoost Classifer can handle categorical data, but this needs to be encoded beforehand. \n",
    "\n",
    "Another option is to remove them entirely since we have all of the information we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from data/new_sm_train\n",
    "data = pd.read_csv('../data/new_sm_train.csv')\n",
    "\n",
    "## Below is using encoded customer_id and vendor_id ##\n",
    "# Initilize LabelEncoder\n",
    "le_vendor = LabelEncoder()\n",
    "\n",
    "# Encode vendor_id\n",
    "data['vendor_id_encoded'] = le_vendor.fit_transform(data['vendor_id'])\n",
    "\n",
    "# Encode customer_id using mean target encoding\n",
    "mean_target_per_customer = data.groupby('customer_id')['target'].mean()\n",
    "data['customer_id_encoded'] = data['customer_id'].map(round(mean_target_per_customer))\n",
    "\n",
    "# Initialize model with categorical features\n",
    "categorical_features = ['customer_id_encoded', 'vendor_id_encoded']\n",
    "\n",
    "# How categorical features will be included in the model\n",
    "# model = HistGradientBoostingClassifier(categorical_features=categorical_features,max_bins=25,max_iter=10)\n",
    "\n",
    "# Drop original customer_id and vendor_id\n",
    "data = data.drop(columns=['customer_id', 'vendor_id'])\n",
    "\n",
    "## Below is removing customer_id and vendor_id ##\n",
    "# data = data.drop(columns=['customer_id', 'vendor_id'])\n",
    "# categorical_features = []\n",
    "\n",
    "# print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoded the customer and vendor id, we will create a model class to get our evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, model, X_train, X_test, y_train, y_test):\n",
    "        # Store the model and data\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = None\n",
    "        self.y_prob = None\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "    \n",
    "    def get_predictions(self):\n",
    "        return self.y_pred\n",
    "\n",
    "    # Confusion Matrix Visualization\n",
    "    def get_confusion_matrix(self):\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Ordered', 'Ordered'], yticklabels=['Not Ordered', 'Ordered'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    def get_classification_report(self):\n",
    "        print(classification_report(self.y_test, self.y_pred))\n",
    "        \n",
    "    def get_roc(self):\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, self.model.predict_proba(self.X_test)[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(f'ROC AUC: {roc_auc:.3f}')\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    def get_precision_recall(self):\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, self.y_prob)\n",
    "        average_precision = average_precision_score(self.y_test, self.y_prob)\n",
    "        print(f'Average Precision: {average_precision:.3f}')\n",
    "\n",
    "        # Plot Precision-Recall curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (AP = %0.2f)' % average_precision)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.show()\n",
    "        \n",
    "    def cross_val(self, folds=5):\n",
    "        cv_scores = cross_val_score(self.model, self.X_train, self.y_train, cv=folds, scoring='accuracy')\n",
    "        print(f'Cross-Validation Accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}')\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        feature_names = self.X_test.columns\n",
    "        result = permutation_importance(self.model, self.X_test, self.y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "        # Create a DataFrame to display the results\n",
    "        permutation_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': result.importances_mean,\n",
    "            'std': result.importances_std\n",
    "        })\n",
    "\n",
    "        # Multiply the importances by 100 to convert them to percentages\n",
    "        permutation_importance_df['importance'] = permutation_importance_df['importance'] * 100\n",
    "        permutation_importance_df['std'] = permutation_importance_df['std'] * 100\n",
    "\n",
    "        # Sort the features by importance\n",
    "        permutation_importance_df = permutation_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # Print the permutation feature importance\n",
    "        # for each in permutation_importance_df.iterrows():\n",
    "        #     print(each)\n",
    "        # return permutation_importance_df\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(permutation_importance_df['feature'], permutation_importance_df['importance'])\n",
    "        plt.xlabel('Importance (%)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.gca().invert_yaxis()  # Reverse the order to have the most important feature on top\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    def get_accuracy(self):\n",
    "        accuracy = accuracy_score(self.y_test, self.y_pred)\n",
    "        print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate a few different scenarios:\n",
    "- Dataset as is w/o class_weight='balanced' in model\n",
    "- Dataset as is w/ class_weight='balanced' in model\n",
    "- Undersampling the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------Dataset Without Balanced Flag-----------------##\n",
    "# Prepare X and y\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Initialize model\n",
    "model = HistGradientBoostingClassifier(\n",
    "    categorical_features=categorical_features, \n",
    "    max_bins=25, \n",
    "    max_iter=10, \n",
    "    max_depth=5,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Still stratify y to keep the same distribution\n",
    "\n",
    "# Create an instance of ModelEvaluation\n",
    "no_bal_flag_model = ModelEvaluation(model, X_train, X_test, y_train, y_test)\n",
    "no_bal_flag_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------Dataset With Balanced Flag-----------------##\n",
    "# Initialize model\n",
    "model = HistGradientBoostingClassifier(\n",
    "    categorical_features=categorical_features, \n",
    "    class_weight='balanced',\n",
    "    max_bins=25, \n",
    "    max_iter=10, \n",
    "    max_depth=5,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Still stratify y to keep the same distribution\n",
    "\n",
    "# Create an instance of ModelEvaluation\n",
    "bal_flag_model = ModelEvaluation(model, X_train, X_test, y_train, y_test)\n",
    "bal_flag_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------Undersampling the Majority-----------------##\n",
    "# Concatenate X and y for easy manipulation\n",
    "balanced_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Separate the two classes\n",
    "class_0 = balanced_data[balanced_data['target'] == 0]\n",
    "class_1 = balanced_data[balanced_data['target'] == 1]\n",
    "\n",
    "# Undersample the majority class (0)\n",
    "class_0_undersampled = resample(class_0, replace=False, n_samples=len(class_1), random_state=42)\n",
    "\n",
    "# Combine the minority class and the undersampled majority class\n",
    "balanced_data_undersampled = pd.concat([class_0_undersampled, class_1])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data_undersampled = balanced_data_undersampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare X and y\n",
    "X = balanced_data_undersampled.drop(columns=['target'])\n",
    "y = balanced_data_undersampled['target']\n",
    "\n",
    "# Initialize model\n",
    "model = HistGradientBoostingClassifier(\n",
    "    categorical_features=categorical_features, \n",
    "    max_bins=25, \n",
    "    max_iter=10, \n",
    "    max_depth=5,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Still stratify y to keep the same distribution\n",
    "\n",
    "# Create an instance of ModelEvaluation\n",
    "balanced_model = ModelEvaluation(model, X_train, X_test, y_train, y_test)\n",
    "balanced_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all models are trained we loop to compare different evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Without Balanced Flag\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     84034\n",
      "           1       1.00      0.59      0.74      1447\n",
      "\n",
      "    accuracy                           0.99     85481\n",
      "   macro avg       1.00      0.80      0.87     85481\n",
      "weighted avg       0.99      0.99      0.99     85481\n",
      "\n",
      "Evaluation With Balanced Flag\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.95     84034\n",
      "           1       0.14      0.78      0.24      1447\n",
      "\n",
      "    accuracy                           0.91     85481\n",
      "   macro avg       0.57      0.85      0.59     85481\n",
      "weighted avg       0.98      0.91      0.94     85481\n",
      "\n",
      "Evaluation With Balanced Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86      1448\n",
      "           1       0.90      0.78      0.84      1447\n",
      "\n",
      "    accuracy                           0.85      2895\n",
      "   macro avg       0.86      0.85      0.85      2895\n",
      "weighted avg       0.86      0.85      0.85      2895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [no_bal_flag_model, bal_flag_model, balanced_model]\n",
    "model_names = ['Without Balanced Flag', 'With Balanced Flag', 'With Balanced Data']\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f'Evaluation {name}')\n",
    "    # Uncomment out the following lines to get the evaluation\n",
    "    # model.get_accuracy()\n",
    "    # model.get_confusion_matrix()\n",
    "    # model.get_roc()\n",
    "    model.get_classification_report()\n",
    "    # model.get_precision_recall()\n",
    "    # model.cross_val()\n",
    "    # model.get_feature_importance()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we see the most consistent performance with balanced data, we will go through multiple splits of the majority class to compare how it does overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision for Class 1: 0.904\n",
      "Average Recall for Class 1: 0.779\n",
      "Average F1-Score for Class 1: 0.837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "majority_class = data[data['target'] == 0]\n",
    "minority_class = data[data['target'] == 1]\n",
    "\n",
    "# Shuffle the majority class\n",
    "majority_class_shuffled = shuffle(majority_class, random_state=42)\n",
    "\n",
    "minority_class_size = len(minority_class)\n",
    "\n",
    "# Calculate number of splits needed to cover the majority class\n",
    "num_splits = len(majority_class) // minority_class_size\n",
    "\n",
    "# Initialize lists to store evaluation results\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "model = HistGradientBoostingClassifier(categorical_features=categorical_features, max_bins=25, max_iter=10, max_depth=5)\n",
    "\n",
    "# Loop to perform different splits of the majority class\n",
    "for i in range(num_splits):\n",
    "    # Select the subset of the majority class for this split\n",
    "    start_idx = i * minority_class_size\n",
    "    end_idx = (i + 1) * minority_class_size\n",
    "    majority_split = majority_class_shuffled.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Combine the majority split with the full minority class\n",
    "    balanced_data = pd.concat([majority_split, minority_class], axis=0)\n",
    "    \n",
    "    # Split into features (X) and target (y)\n",
    "    X = balanced_data.drop(columns=['target'])\n",
    "    y = balanced_data['target']\n",
    "    \n",
    "    # Split the dataset into train and test sets (80% training, 20% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Store the classification report for evaluation\n",
    "    eval_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision_list.append(eval_report['1']['precision'])\n",
    "    recall_list.append(eval_report['1']['recall'])\n",
    "    f1_list.append(eval_report['1']['f1-score'])\n",
    "\n",
    "# Calculate the average for each metric\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_f1 = np.mean(f1_list)\n",
    "\n",
    "# Print the averages\n",
    "print(f\"Average Precision for Class 1: {avg_precision:.3f}\")\n",
    "print(f\"Average Recall for Class 1: {avg_recall:.3f}\")\n",
    "print(f\"Average F1-Score for Class 1: {avg_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
