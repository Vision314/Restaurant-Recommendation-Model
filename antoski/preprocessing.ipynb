{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Data Analysis Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/xpbsmtsx0xzbs7_6z34b_5yr0000gn/T/ipykernel_55469/3449389243.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_full = pd.read_csv('../data/train_full.csv')\n",
      "/var/folders/9t/xpbsmtsx0xzbs7_6z34b_5yr0000gn/T/ipykernel_55469/3449389243.py:5: DtypeWarning: Columns (15,16,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  orders = pd.read_csv('../data/orders.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "train_full\n",
      "(5802400, 73)\n",
      "train_customers\n",
      "(34674, 8)\n",
      "train_locations\n",
      "(59503, 5)\n",
      "orders\n",
      "(135303, 26)\n",
      "vendors\n",
      "(100, 59)\n",
      "sample_submission\n",
      "(1672000, 2)\n",
      "Data columns:\n",
      "train_full\n",
      "Index(['customer_id', 'gender', 'status_x', 'verified_x', 'created_at_x',\n",
      "       'updated_at_x', 'location_number', 'location_type', 'latitude_x',\n",
      "       'longitude_x', 'id', 'authentication_id', 'latitude_y', 'longitude_y',\n",
      "       'vendor_category_en', 'vendor_category_id', 'delivery_charge',\n",
      "       'serving_distance', 'is_open', 'OpeningTime', 'OpeningTime2',\n",
      "       'prepration_time', 'commission', 'is_akeed_delivering',\n",
      "       'discount_percentage', 'status_y', 'verified_y', 'rank', 'language',\n",
      "       'vendor_rating', 'sunday_from_time1', 'sunday_to_time1',\n",
      "       'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
      "       'monday_to_time1', 'monday_from_time2', 'monday_to_time2',\n",
      "       'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
      "       'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1',\n",
      "       'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1',\n",
      "       'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2',\n",
      "       'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
      "       'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1',\n",
      "       'saturday_from_time2', 'saturday_to_time2', 'primary_tags',\n",
      "       'open_close_flags', 'vendor_tag', 'vendor_tag_name', 'one_click_vendor',\n",
      "       'country_id', 'city_id', 'created_at_y', 'updated_at_y', 'device_type',\n",
      "       'display_orders', 'location_number_obj', 'id_obj',\n",
      "       'CID X LOC_NUM X VENDOR', 'target'],\n",
      "      dtype='object')\n",
      "train_customers\n",
      "Index(['akeed_customer_id', 'gender', 'dob', 'status', 'verified', 'language',\n",
      "       'created_at', 'updated_at'],\n",
      "      dtype='object')\n",
      "train_locations\n",
      "Index(['customer_id', 'location_number', 'location_type', 'latitude',\n",
      "       'longitude'],\n",
      "      dtype='object')\n",
      "orders\n",
      "Index(['akeed_order_id', 'customer_id', 'item_count', 'grand_total',\n",
      "       'payment_mode', 'promo_code', 'vendor_discount_amount',\n",
      "       'promo_code_discount_percentage', 'is_favorite', 'is_rated',\n",
      "       'vendor_rating', 'driver_rating', 'deliverydistance', 'preparationtime',\n",
      "       'delivery_time', 'order_accepted_time', 'driver_accepted_time',\n",
      "       'ready_for_pickup_time', 'picked_up_time', 'delivered_time',\n",
      "       'delivery_date', 'vendor_id', 'created_at', 'LOCATION_NUMBER',\n",
      "       'LOCATION_TYPE', 'CID X LOC_NUM X VENDOR'],\n",
      "      dtype='object')\n",
      "vendors\n",
      "Index(['id', 'authentication_id', 'latitude', 'longitude',\n",
      "       'vendor_category_en', 'vendor_category_id', 'delivery_charge',\n",
      "       'serving_distance', 'is_open', 'OpeningTime', 'OpeningTime2',\n",
      "       'prepration_time', 'commission', 'is_akeed_delivering',\n",
      "       'discount_percentage', 'status', 'verified', 'rank', 'language',\n",
      "       'vendor_rating', 'sunday_from_time1', 'sunday_to_time1',\n",
      "       'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
      "       'monday_to_time1', 'monday_from_time2', 'monday_to_time2',\n",
      "       'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
      "       'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1',\n",
      "       'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1',\n",
      "       'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2',\n",
      "       'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
      "       'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1',\n",
      "       'saturday_from_time2', 'saturday_to_time2', 'primary_tags',\n",
      "       'open_close_flags', 'vendor_tag', 'vendor_tag_name', 'one_click_vendor',\n",
      "       'country_id', 'city_id', 'created_at', 'updated_at', 'device_type',\n",
      "       'display_orders'],\n",
      "      dtype='object')\n",
      "sample_submission\n",
      "Index(['CID X LOC_NUM X VENDOR', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import all training data and vendor, order, and customer data\n",
    "train_full = pd.read_csv('../data/train_full.csv')\n",
    "train_customers = pd.read_csv('../data/train_customers.csv')\n",
    "train_locations = pd.read_csv('../data/train_locations.csv')\n",
    "orders = pd.read_csv('../data/orders.csv')\n",
    "vendors = pd.read_csv('../data/vendors.csv')\n",
    "sample_submission = pd.read_csv('../data/SampleSubmission.csv')\n",
    "\n",
    "# List of all datasets and names\n",
    "data_collection = [train_full, train_customers, train_locations, orders, vendors, sample_submission]\n",
    "data_names = ['train_full', 'train_customers', 'train_locations', 'orders', 'vendors', 'sample_submission']\n",
    "\n",
    "# Display shape of each dataset\n",
    "print(\"Data shapes:\")\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(name)\n",
    "    print(data.shape)\n",
    "    \n",
    "# Display column names for each data set\n",
    "print(\"Data columns:\")\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(name)\n",
    "    print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- train_full is a combination of vendors, and orders with the ultimate result being 1. In our scenario where we want to recommend a restaurant to a person based off previous order history, we may be able to ignore many of the features presented. However, the one unique column is target. By assumption, it would seem that we need to classify yes/no (1/0) based on the rest of the information given here.\n",
    "- train_customers is mostly useless. The majority of the information is either missing or very similar. There could be some analysis given gender (22k) or dob (3k).\n",
    "- train_locations can potentially be useful. It lists the number of locations each customer has, although the majority only has one location and they are unlabeled so we would need to make assumptions.\n",
    "- orders will defitinely be helpful since this is the basis of our project.\n",
    "- vendors will also be helpful in understanding the type of restaurant and being able to filter out recommendations based on location and time of day.\n",
    "- After further analysis, it is clear that sample_submission is the given file containing the customer, order location (home, work, other), and a vendor. Given this file we need to decide if the customer would order here or not. (1/0)\n",
    "- This brings up the question if we would be then only focusing on if someone would order from a vendor, if we are trying to recommend a number of vendors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train_full\n",
      "commission\n",
      "is_akeed_delivering\n",
      "language\n",
      "open_close_flags\n",
      "one_click_vendor\n",
      "country_id\n",
      "city_id\n",
      "display_orders\n",
      "Dataset: train_customers\n",
      "language\n",
      "Dataset: train_locations\n",
      "Dataset: orders\n",
      "Dataset: vendors\n",
      "commission\n",
      "is_akeed_delivering\n",
      "language\n",
      "open_close_flags\n",
      "one_click_vendor\n",
      "country_id\n",
      "city_id\n",
      "display_orders\n",
      "Dataset: sample_submission\n",
      "target\n",
      "Dataset: train_full\n",
      "is_open\n",
      "status_y\n",
      "verified_y\n",
      "device_type\n",
      "Dataset: train_customers\n",
      "Dataset: train_locations\n",
      "Dataset: orders\n",
      "Dataset: vendors\n",
      "is_open\n",
      "status\n",
      "verified\n",
      "device_type\n",
      "Dataset: sample_submission\n"
     ]
    }
   ],
   "source": [
    "# For each dataset, we check if the number of unique values is one for each column. If so, we print the column name and add to list.\n",
    "drop_columns = []  \n",
    "\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(f'Dataset: {name}')\n",
    "    for column in data.columns:\n",
    "        # Check if the column only has one unique value\n",
    "        if data[column].nunique() == 1:\n",
    "            drop_columns.append(column)\n",
    "            print(column)\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "            \n",
    "                \n",
    "# Now we check if we drop all missing values, if the number of unique values is one for each column. If so, we print the column name.\n",
    "for name, data in zip(data_names, data_collection):\n",
    "    print(f'Dataset: {name}')\n",
    "    temp_data = data.dropna()\n",
    "    for column in temp_data.columns:\n",
    "        # Check if the column only has one unique value\n",
    "        if temp_data[column].nunique() == 1:\n",
    "            drop_columns.append(column)\n",
    "            print(column)\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full datasets: \n",
    "train_full: \n",
    "- commission\n",
    "- is_akeed_delivering\n",
    "- language\n",
    "- open_close_flags\n",
    "- one_click_vendor\n",
    "- country_id\n",
    "- city_id\n",
    "- display_orders\n",
    "train_customers:\n",
    "- language\n",
    "vendors: \n",
    "- commission\n",
    "- is_akeed_delivering\n",
    "- language\n",
    "- open_close_flags\n",
    "- one_click_vendor\n",
    "- country_id\n",
    "- city_id\n",
    "- display_orders\n",
    "\n",
    "Dropped missing values (unique from previous)\n",
    "train_full:\n",
    "- is_open\n",
    "- status_y\n",
    "- verified_y\n",
    "- one_click_vendor\n",
    "- device_type\n",
    "Dataset: vendors\n",
    "- is_open\n",
    "- status\n",
    "- verified\n",
    "- one_click_vendor\n",
    "- device_type \n",
    "\n",
    "Notes:\n",
    "- There are a total of 8 columns that only contain one unique entry between train_full and vendors. Train_customers contains one of these as well. \n",
    "- When dropping missing entries, we have another 5 columns between train_full and vendors with only one unique entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target', 'status_y', 'status', 'language', 'country_id', 'is_open', 'is_akeed_delivering', 'verified', 'verified_y', 'device_type', 'display_orders', 'one_click_vendor', 'city_id', 'commission', 'open_close_flags']\n",
      "15\n",
      "Counts for target:\n",
      "target\n",
      "0    5724146\n",
      "1      78254\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analysis of the columns that have only one unique value\n",
    "drop_columns = list(set(drop_columns))\n",
    "print(drop_columns)\n",
    "print(len(drop_columns))\n",
    "\n",
    "# Target Counts\n",
    "target_counts = train_full['target'].value_counts()\n",
    "print(\"Counts for target:\")\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data shapes:\n",
      "(78254, 61)\n",
      "    customer_id gender  status_x  verified_x         created_at_x  \\\n",
      "56      TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "227     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "362     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "370     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "404     ZGFSYCZ   Male         1           1  2018-02-09 12:04:42   \n",
      "\n",
      "            updated_at_x  location_number location_type  latitude_x  \\\n",
      "56   2018-02-07 19:16:23                0          Work    -96.4400   \n",
      "227  2018-02-07 19:16:23                2           NaN     -0.1287   \n",
      "362  2018-02-09 12:04:41                0          Home     -0.1755   \n",
      "370  2018-02-09 12:04:41                0          Home     -0.1755   \n",
      "404  2018-02-09 12:04:41                1          Home      0.1912   \n",
      "\n",
      "     longitude_x  ...  saturday_to_time2             primary_tags  \\\n",
      "56        -67.20  ...           23:45:00                      NaN   \n",
      "227       -78.56  ...                NaN   {\"primary_tags\":\"462\"}   \n",
      "362       -78.56  ...                NaN   {\"primary_tags\":\"180\"}   \n",
      "370       -78.56  ...                NaN  {\"primary_tags\":\"1088\"}   \n",
      "404       -78.60  ...           23:59:00    {\"primary_tags\":\"32\"}   \n",
      "\n",
      "               vendor_tag                                    vendor_tag_name  \\\n",
      "56   1,5,8,57,30,27,24,16  American,Burgers,Desserts,Donuts,Fries,Pasta,S...   \n",
      "227             2,8,91,10               Arabic,Desserts,Free Delivery,Indian   \n",
      "362                 41,57                                       Cakes,Donuts   \n",
      "370        60,38,61,16,36  Coffee,Fresh Juices,Hot Chocolate,Sandwiches,S...   \n",
      "404                     5                                            Burgers   \n",
      "\n",
      "            created_at_y         updated_at_y  location_number_obj  id_obj  \\\n",
      "56   2019-04-30 16:15:30  2020-04-07 18:45:33                    0     237   \n",
      "227  2018-10-24 18:11:35  2020-04-07 23:51:01                    2     113   \n",
      "362  2019-05-31 14:33:35  2020-04-06 19:57:22                    0     274   \n",
      "370  2019-07-02 14:50:31  2020-03-31 13:23:57                    0     303   \n",
      "404  2018-05-17 22:12:38  2020-04-05 15:57:41                    1      28   \n",
      "\n",
      "    CID X LOC_NUM X VENDOR target  \n",
      "56       TCHWPBT X 0 X 237      1  \n",
      "227      TCHWPBT X 2 X 113      1  \n",
      "362      ZGFSYCZ X 0 X 274      1  \n",
      "370      ZGFSYCZ X 0 X 303      1  \n",
      "404       ZGFSYCZ X 1 X 28      1  \n",
      "\n",
      "[5 rows x 61 columns]\n",
      "Index(['customer_id', 'gender', 'status_x', 'verified_x', 'created_at_x',\n",
      "       'updated_at_x', 'location_number', 'location_type', 'latitude_x',\n",
      "       'longitude_x', 'id', 'authentication_id', 'latitude_y', 'longitude_y',\n",
      "       'vendor_category_en', 'vendor_category_id', 'delivery_charge',\n",
      "       'serving_distance', 'OpeningTime', 'OpeningTime2', 'prepration_time',\n",
      "       'discount_percentage', 'rank', 'vendor_rating', 'sunday_from_time1',\n",
      "       'sunday_to_time1', 'sunday_from_time2', 'sunday_to_time2',\n",
      "       'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
      "       'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1',\n",
      "       'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time1',\n",
      "       'wednesday_to_time1', 'wednesday_from_time2', 'wednesday_to_time2',\n",
      "       'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
      "       'thursday_to_time2', 'friday_from_time1', 'friday_to_time1',\n",
      "       'friday_from_time2', 'friday_to_time2', 'saturday_from_time1',\n",
      "       'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2',\n",
      "       'primary_tags', 'vendor_tag', 'vendor_tag_name', 'created_at_y',\n",
      "       'updated_at_y', 'location_number_obj', 'id_obj',\n",
      "       'CID X LOC_NUM X VENDOR', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Filtering to further understand dataset\n",
    "filtered_full = train_full[train_full['target'] == 1]\n",
    "\n",
    "print(\"Filtered data shapes:\")\n",
    "print(filtered_full.shape)\n",
    "print(filtered_full.head())\n",
    "print(filtered_full.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is further analysis of features in train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping missing values: (5802400, 61)\n",
      "Counts for target: \n",
      " target\n",
      "0    5724146\n",
      "1      78254\n",
      "Name: count, dtype: int64\n",
      "Columns with missing values:\n",
      "customer_id                     0\n",
      "gender                    1705100\n",
      "status_x                        0\n",
      "verified_x                      0\n",
      "created_at_x                    0\n",
      "updated_at_x                    0\n",
      "location_number                 0\n",
      "location_type             2654200\n",
      "latitude_x                    600\n",
      "longitude_x                   600\n",
      "id                              0\n",
      "authentication_id               0\n",
      "latitude_y                      0\n",
      "longitude_y                     0\n",
      "vendor_category_en              0\n",
      "vendor_category_id              0\n",
      "delivery_charge                 0\n",
      "serving_distance                0\n",
      "OpeningTime                522216\n",
      "OpeningTime2               522216\n",
      "prepration_time                 0\n",
      "discount_percentage             0\n",
      "rank                            0\n",
      "vendor_rating                   0\n",
      "sunday_from_time1           58024\n",
      "sunday_to_time1             58024\n",
      "sunday_from_time2         3365392\n",
      "sunday_to_time2           3365392\n",
      "monday_from_time1               0\n",
      "monday_to_time1                 0\n",
      "monday_from_time2         3365392\n",
      "monday_to_time2           3365392\n",
      "tuesday_from_time1          58024\n",
      "tuesday_to_time1            58024\n",
      "tuesday_from_time2        3423416\n",
      "tuesday_to_time2          3423416\n",
      "wednesday_from_time1            0\n",
      "wednesday_to_time1              0\n",
      "wednesday_from_time2      3365392\n",
      "wednesday_to_time2        3365392\n",
      "thursday_from_time1         58024\n",
      "thursday_to_time1           58024\n",
      "thursday_from_time2       3365392\n",
      "thursday_to_time2         3365392\n",
      "friday_from_time1          232096\n",
      "friday_to_time1            232096\n",
      "friday_from_time2         3191320\n",
      "friday_to_time2           3191320\n",
      "saturday_from_time1        116048\n",
      "saturday_to_time1          116048\n",
      "saturday_from_time2       3365392\n",
      "saturday_to_time2         3365392\n",
      "primary_tags              1334552\n",
      "vendor_tag                 174072\n",
      "vendor_tag_name            174072\n",
      "created_at_y                    0\n",
      "updated_at_y                    0\n",
      "location_number_obj             0\n",
      "id_obj                          0\n",
      "CID X LOC_NUM X VENDOR          0\n",
      "target                          0\n",
      "  customer_id gender  status_x  verified_x         created_at_x  \\\n",
      "0     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "1     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "2     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "3     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "4     TCHWPBT   Male         1           1  2018-02-07 19:16:23   \n",
      "\n",
      "          updated_at_x  location_number location_type  latitude_x  \\\n",
      "0  2018-02-07 19:16:23                0          Work      -96.44   \n",
      "1  2018-02-07 19:16:23                0          Work      -96.44   \n",
      "2  2018-02-07 19:16:23                0          Work      -96.44   \n",
      "3  2018-02-07 19:16:23                0          Work      -96.44   \n",
      "4  2018-02-07 19:16:23                0          Work      -96.44   \n",
      "\n",
      "   longitude_x  ...  saturday_to_time2           primary_tags  \\\n",
      "0        -67.2  ...           23:59:00   {\"primary_tags\":\"4\"}   \n",
      "1        -67.2  ...           23:59:00   {\"primary_tags\":\"7\"}   \n",
      "2        -67.2  ...                NaN  {\"primary_tags\":\"71\"}   \n",
      "3        -67.2  ...                NaN  {\"primary_tags\":\"46\"}   \n",
      "4        -67.2  ...           23:59:00  {\"primary_tags\":\"32\"}   \n",
      "\n",
      "                  vendor_tag  \\\n",
      "0  2,4,5,8,91,22,12,24,16,23   \n",
      "1  4,41,51,34,27,15,24,16,28   \n",
      "2                  4,8,91,10   \n",
      "3                  5,8,30,24   \n",
      "4                          5   \n",
      "\n",
      "                                     vendor_tag_name         created_at_y  \\\n",
      "0  Arabic,Breakfast,Burgers,Desserts,Free Deliver...  2018-01-30 14:42:04   \n",
      "1  Breakfast,Cakes,Crepes,Italian,Pasta,Pizzas,Sa...  2018-05-03 12:32:06   \n",
      "2            Breakfast,Desserts,Free Delivery,Indian  2018-05-04 22:28:22   \n",
      "3                      Burgers,Desserts,Fries,Salads  2018-05-06 19:20:48   \n",
      "4                                            Burgers  2018-05-17 22:12:38   \n",
      "\n",
      "          updated_at_y  location_number_obj  id_obj CID X LOC_NUM X VENDOR  \\\n",
      "0  2020-04-07 15:12:43                    0       4        TCHWPBT X 0 X 4   \n",
      "1  2020-04-05 20:46:03                    0      13       TCHWPBT X 0 X 13   \n",
      "2  2020-04-07 16:35:55                    0      20       TCHWPBT X 0 X 20   \n",
      "3  2020-04-02 00:56:17                    0      23       TCHWPBT X 0 X 23   \n",
      "4  2020-04-05 15:57:41                    0      28       TCHWPBT X 0 X 28   \n",
      "\n",
      "  target  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "# Going back to look at the train_full dataset\n",
    "print(f'Before dropping missing values: {train_full.shape}')\n",
    "\n",
    "# Look at how many targets are 1 and 0\n",
    "target_counts = train_full['target'].value_counts()\n",
    "print(f'Counts for target: \\n {target_counts}')\n",
    "\n",
    "# Check how many columns contain missing values in train_full\n",
    "print(\"Columns with missing values:\")\n",
    "missing_values = train_full.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(missing_values.to_string())\n",
    "    \n",
    "print(train_full.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 5.7 million training samples, but of those only 78k have a target = 1. We will need to be careful with removing rows since we want to retain as many targets = 1, which is our \"recommended\" flag. \n",
    "\n",
    "We know we want to remove a lot of columns since there is a lot of information for our purposes that are unnessecary. \n",
    "- Gender: This column could be useful for gender analysis, but for ordering food we can remove this. \n",
    "- Location Type: This is the stirng label for the customer's location. Most have 1 location (most likely home), we can ignore this.\n",
    "- Latitude_x and longitude_x: The customer's location, we may be able to drop the rows with missing values. This would be important to consider if a vendor will deliver to a customer's location. \n",
    "- Opening Time/Hours of Operation: These columns would be beneficial if we were considering TOD when ordering. For our purposes we can safely drop these columns. (Monday and Wednesday have no missing values, but we will still remove)\n",
    "- Primary Tag: This column is inconsistent with the values found in vendor_tag, for this reason we will drop the column\n",
    "- Vendor Tag/Vendor Tag Name: There are three vendors lacking vendor tags, but these vendors may come up in test time. Because of this we will need to work around the missing labels. \n",
    "\n",
    "_Note: we have already removed columns with only one unique entry_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'location_type', 'latitude_x', 'longitude_x', 'OpeningTime', 'OpeningTime2', 'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2', 'sunday_to_time2', 'monday_from_time2', 'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2', 'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2', 'primary_tags', 'vendor_tag', 'vendor_tag_name']\n",
      "After dropping columns with missing values: (5802400, 28)\n",
      "Counts for target:\n",
      "target\n",
      "0    5724146\n",
      "1      78254\n",
      "Name: count, dtype: int64\n",
      "Index(['customer_id', 'status_x', 'verified_x', 'created_at_x', 'updated_at_x',\n",
      "       'location_number', 'latitude_x', 'longitude_x', 'id',\n",
      "       'authentication_id', 'latitude_y', 'longitude_y', 'vendor_category_en',\n",
      "       'vendor_category_id', 'delivery_charge', 'serving_distance',\n",
      "       'prepration_time', 'discount_percentage', 'rank', 'vendor_rating',\n",
      "       'vendor_tag', 'vendor_tag_name', 'created_at_y', 'updated_at_y',\n",
      "       'location_number_obj', 'id_obj', 'CID X LOC_NUM X VENDOR', 'target'],\n",
      "      dtype='object')\n",
      "  customer_id  status_x  verified_x         created_at_x         updated_at_x  \\\n",
      "0     TCHWPBT         1           1  2018-02-07 19:16:23  2018-02-07 19:16:23   \n",
      "1     TCHWPBT         1           1  2018-02-07 19:16:23  2018-02-07 19:16:23   \n",
      "2     TCHWPBT         1           1  2018-02-07 19:16:23  2018-02-07 19:16:23   \n",
      "3     TCHWPBT         1           1  2018-02-07 19:16:23  2018-02-07 19:16:23   \n",
      "4     TCHWPBT         1           1  2018-02-07 19:16:23  2018-02-07 19:16:23   \n",
      "\n",
      "   location_number  latitude_x  longitude_x  id  authentication_id  ...  rank  \\\n",
      "0                0      -96.44        -67.2   4           118597.0  ...    11   \n",
      "1                0      -96.44        -67.2  13           118608.0  ...    11   \n",
      "2                0      -96.44        -67.2  20           118616.0  ...     1   \n",
      "3                0      -96.44        -67.2  23           118619.0  ...    11   \n",
      "4                0      -96.44        -67.2  28           118624.0  ...    11   \n",
      "\n",
      "   vendor_rating                 vendor_tag  \\\n",
      "0            4.4  2,4,5,8,91,22,12,24,16,23   \n",
      "1            4.7  4,41,51,34,27,15,24,16,28   \n",
      "2            4.5                  4,8,91,10   \n",
      "3            4.5                  5,8,30,24   \n",
      "4            4.4                          5   \n",
      "\n",
      "                                     vendor_tag_name         created_at_y  \\\n",
      "0  Arabic,Breakfast,Burgers,Desserts,Free Deliver...  2018-01-30 14:42:04   \n",
      "1  Breakfast,Cakes,Crepes,Italian,Pasta,Pizzas,Sa...  2018-05-03 12:32:06   \n",
      "2            Breakfast,Desserts,Free Delivery,Indian  2018-05-04 22:28:22   \n",
      "3                      Burgers,Desserts,Fries,Salads  2018-05-06 19:20:48   \n",
      "4                                            Burgers  2018-05-17 22:12:38   \n",
      "\n",
      "          updated_at_y  location_number_obj  id_obj  CID X LOC_NUM X VENDOR  \\\n",
      "0  2020-04-07 15:12:43                    0       4         TCHWPBT X 0 X 4   \n",
      "1  2020-04-05 20:46:03                    0      13        TCHWPBT X 0 X 13   \n",
      "2  2020-04-07 16:35:55                    0      20        TCHWPBT X 0 X 20   \n",
      "3  2020-04-02 00:56:17                    0      23        TCHWPBT X 0 X 23   \n",
      "4  2020-04-05 15:57:41                    0      28        TCHWPBT X 0 X 28   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create list of columns with missing values\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index.tolist()\n",
    "print(columns_with_missing_values)\n",
    "\n",
    "# Modified print out from above to include monday and wednesday time1 values and excluse latitude and longitude.\n",
    "cols_to_remove =['gender', 'location_type', 'OpeningTime', 'OpeningTime2', 'sunday_from_time1', \n",
    "                 'sunday_to_time1', 'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1',\n",
    "                 'monday_to_time1','monday_from_time2', 'monday_to_time2', 'tuesday_from_time1', \n",
    "                 'tuesday_to_time1', 'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time1',\n",
    "                 'wednesday_to_time1','wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1', \n",
    "                 'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2', 'friday_from_time1', \n",
    "                 'friday_to_time1', 'friday_from_time2', 'friday_to_time2', 'saturday_from_time1', \n",
    "                 'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2', 'primary_tags']\n",
    "\n",
    "# Lets look at the dataset after dropping all columns above\n",
    "temp_train_full = train_full.drop(columns=cols_to_remove)\n",
    "print(f'After dropping columns with missing values: {temp_train_full.shape}')\n",
    "target_counts = temp_train_full['target'].value_counts()\n",
    "print(\"Counts for target:\")\n",
    "print(target_counts)\n",
    "print(temp_train_full.columns)\n",
    "print(temp_train_full.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from looking at CSV and intuition:\n",
    "- Customer ID: Keep \n",
    "- Status/Verified Columns: Unsure\n",
    "- Created At/Updated At Columns: Drop as many across the entry do not change. \n",
    "- Location Number: Identical to Location obj, representing customer location. Rename and keep one. \n",
    "- Latitude/Longitude X/Y: Keep\n",
    "- ID: Identical to ID obj, representing vendor ID. Rename and keep one. \n",
    "- Authentication ID: Unsure\n",
    "- Vendor Category EN/#: Drop EN, keep # and transform to binary.\n",
    "- Delivery Charge: Keep\n",
    "- Serving Distance: Keep\n",
    "- Preperation Time: Keep\n",
    "- Discount Percentage: Keep\n",
    "- Rank: Drop, only two values 1 and 11. \n",
    "- Vendor Rating: Keep\n",
    "- Vendor Tag/Names: Keep and modifiy \n",
    "- Location Number obj: Drop, reason above ^\n",
    "- ID obj: Drop, reason above ^\n",
    "- CID X LOC_NUM X VENDOR: Keep, but this may be removed for training. \n",
    "- target: Keep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns after further analysis\n",
    "drop_cols = ['status_x', 'verified_x', 'created_at_x', 'updated_at_x',\n",
    "       'authentication_id','vendor_category_en',\n",
    "       'rank', 'created_at_y', 'updated_at_y']\n",
    "\n",
    "temp_train_full.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Drop duplicate columns and rename remaining columns\n",
    "temp_train_full.drop(columns=['location_number_obj', 'id_obj'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer: QFWLNUK\n",
    "(500, 17)\n",
    "Customer: NSQRO1H\n",
    "(400, 17)\n",
    "Customer: VZIK43C\n",
    "(400, 17)\n",
    "Customer: O0LALCF\n",
    "(100, 17)\n",
    "Customer: 7URX8JP\n",
    "(300, 17)\n",
    "Customer: 55MCNEF\n",
    "(200, 17)\n",
    "\n",
    "Customer: QFWLNUK\n",
    "(400, 17)\n",
    "Customer: NSQRO1H\n",
    "(300, 17)\n",
    "Customer: VZIK43C\n",
    "(300, 17)\n",
    "Customer: O0LALCF\n",
    "Customer not found\n",
    "Customer: 7URX8JP\n",
    "(200, 17)\n",
    "Customer: 55MCNEF\n",
    "(100, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing latitude and longitude values: (600, 17)\n",
      "Unique customers with missing latitude and longitude values: 6\n",
      "['QFWLNUK' 'NSQRO1H' 'VZIK43C' 'O0LALCF' '7URX8JP' '55MCNEF']\n"
     ]
    }
   ],
   "source": [
    "# Now lets look back at the entries with missing longitude and latitude values\n",
    "missing_lat_long = temp_train_full[temp_train_full['latitude_x'].isnull() | temp_train_full['longitude_x'].isnull()]\n",
    "print(f'Missing latitude and longitude values: {missing_lat_long.shape}')\n",
    "\n",
    "# Get the list of customers with missing latitude and longitude values\n",
    "missing_lat_long_customers = missing_lat_long['customer_id'].unique()\n",
    "print(f'Unique customers with missing latitude and longitude values: {len(missing_lat_long_customers)}')\n",
    "print(missing_lat_long_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 34k+ customers, 6 lack latitude and longitude. Lets see if other rows are present for them without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency in train_full\n",
      "Customer: QFWLNUK\n",
      "(500, 17)\n",
      "Customer: NSQRO1H\n",
      "(400, 17)\n",
      "Customer: VZIK43C\n",
      "(400, 17)\n",
      "Customer: O0LALCF\n",
      "(100, 17)\n",
      "Customer: 7URX8JP\n",
      "(300, 17)\n",
      "Customer: 55MCNEF\n",
      "(200, 17)\n",
      "Frequency in train_customers\n",
      "Customer: QFWLNUK\n",
      "(1, 7)\n",
      "Customer: NSQRO1H\n",
      "(1, 7)\n",
      "Customer: VZIK43C\n",
      "(1, 7)\n",
      "Customer: O0LALCF\n",
      "(1, 7)\n",
      "Customer: 7URX8JP\n",
      "(1, 7)\n",
      "Customer: 55MCNEF\n",
      "(1, 7)\n",
      "Frequency in train_locations\n",
      "Customer: QFWLNUK\n",
      "(5, 5)\n",
      "Customer: NSQRO1H\n",
      "(4, 5)\n",
      "Customer: VZIK43C\n",
      "(4, 5)\n",
      "Customer: O0LALCF\n",
      "(1, 5)\n",
      "Customer: 7URX8JP\n",
      "(3, 5)\n",
      "Customer: 55MCNEF\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "# Lets see if our list of missing valued customers is still present otherwise in train_full\n",
    "missing_ll_customers = ['QFWLNUK', 'NSQRO1H', 'VZIK43C', 'O0LALCF', '7URX8JP', '55MCNEF']\n",
    "\n",
    "print('Frequency in train_full')\n",
    "for customer in missing_ll_customers:\n",
    "    print(f'Customer: {customer}')\n",
    "    if temp_train_full[temp_train_full['customer_id'] == customer].shape[0] == 0:\n",
    "        print('Customer not found')\n",
    "    else:\n",
    "        print(temp_train_full[temp_train_full['customer_id'] == customer].shape)\n",
    "    \n",
    "# Lets see if the missing entries are in train_customers\n",
    "print('Frequency in train_customers')\n",
    "for customer in missing_ll_customers:\n",
    "    print(f'Customer: {customer}')\n",
    "    if train_customers[train_customers['akeed_customer_id'] == customer].shape[0] == 0:\n",
    "        print('Customer not found')\n",
    "    else:\n",
    "        print(train_customers[train_customers['akeed_customer_id'] == customer].shape)\n",
    "        \n",
    "# Lets see if the missing entries are in train_locations\n",
    "print('Frequency in train_locations')\n",
    "for customer in missing_ll_customers:\n",
    "    print(f'Customer: {customer}')\n",
    "    if train_locations[train_locations['customer_id'] == customer].shape[0] == 0:\n",
    "        print('Customer not found')\n",
    "    else:\n",
    "        print(train_locations[train_locations['customer_id'] == customer].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know we have 600 rows missing latitude_x and longitude_x, and here we can see across the 6 customers there's a total of 1900 rows, we can safely remove the missing entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for target:\n",
      "target\n",
      "0    5724146\n",
      "1      78254\n",
      "Name: count, dtype: int64\n",
      "After removing rows with missing latitude and longitude values: (5801800, 17)\n",
      "Counts for target:\n",
      "target\n",
      "0    5723551\n",
      "1      78249\n",
      "Name: count, dtype: int64\n",
      "Customer: QFWLNUK\n",
      "(400, 17)\n",
      "Customer: NSQRO1H\n",
      "(300, 17)\n",
      "Customer: VZIK43C\n",
      "(300, 17)\n",
      "Customer: O0LALCF\n",
      "Customer not found\n",
      "Customer: 7URX8JP\n",
      "(200, 17)\n",
      "Customer: 55MCNEF\n",
      "(100, 17)\n"
     ]
    }
   ],
   "source": [
    "# Origial target outputs\n",
    "print(\"Counts for target:\")\n",
    "target_counts = temp_train_full['target'].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "# After removing rows with missing latitude and longitude values\n",
    "temp_train_full.dropna(subset=['latitude_x', 'longitude_x'], inplace=True)\n",
    "print(f'After removing rows with missing latitude and longitude values: {temp_train_full.shape}')\n",
    "target_counts = temp_train_full['target'].value_counts()\n",
    "print(\"Counts for target:\")\n",
    "print(target_counts)\n",
    "\n",
    "# Again look at the customer's with missing latitude and longitude values\n",
    "for customer in missing_ll_customers:\n",
    "    print(f'Customer: {customer}')\n",
    "    if temp_train_full[temp_train_full['customer_id'] == customer].shape[0] == 0:\n",
    "        print('Customer not found')\n",
    "    else:\n",
    "        print(temp_train_full[temp_train_full['customer_id'] == customer].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we lose 5 cases where one of these customers chooses a vendor. Hopefully there is enough information otherwise for them to give them a recommendation. \n",
    "\n",
    "Customer O0LALCF was completely removed from train_full. On test_full, we will be applying all of the same preprocessing done here to keep the datasets consistent with information. \n",
    "\n",
    "Final step is to only include rows where location_number = 0 and then remove the 'location_number' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for target:\n",
      "target\n",
      "0    3411069\n",
      "1      40931\n",
      "Name: count, dtype: int64\n",
      "(3452000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Drop location_number != 0 from train_full\n",
    "temp_train_full = temp_train_full[temp_train_full['location_number'] == 0]\n",
    "\n",
    "# Drop location_number column \n",
    "temp_train_full.drop(columns=['location_number'], inplace=True)\n",
    "\n",
    "print(temp_train_full.shape)\n",
    "\n",
    "# Save preprocessed data to csv\n",
    "temp_train_full.to_csv('../data/preprocessed/train_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "33\n",
      "9\n",
      "55\n",
      "['is_open', 'status_y', 'device_type', 'verified_y', 'commission', 'is_akeed_delivering', 'language', 'open_close_flags', 'one_click_vendor', 'country_id', 'city_id', 'display_orders', 'gender', 'location_type', 'OpeningTime', 'OpeningTime2', 'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2', 'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2', 'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2', 'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2', 'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2', 'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2', 'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2', 'primary_tags', 'status_x', 'verified_x', 'created_at_x', 'updated_at_x', 'authentication_id', 'vendor_category_en', 'rank', 'created_at_y', 'updated_at_y', 'location_number']\n"
     ]
    }
   ],
   "source": [
    "drop_columns = [\"is_open\", \"status_y\", \"device_type\", 'verified_y',\"commission\", \"is_akeed_delivering\", \"language\", \"open_close_flags\", \"one_click_vendor\", \"country_id\", \"city_id\", \"display_orders\"]\n",
    "\n",
    "# Lists of dropped columns: drop_columns, cols_to_remove, drop_cols\n",
    "columns_to_drop = drop_columns + cols_to_remove + drop_cols\n",
    "columns_to_drop.append('location_number')\n",
    "print(len(drop_columns))\n",
    "print(len(cols_to_remove))\n",
    "print(len(drop_cols))\n",
    "print(len(columns_to_drop))\n",
    "print(columns_to_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
